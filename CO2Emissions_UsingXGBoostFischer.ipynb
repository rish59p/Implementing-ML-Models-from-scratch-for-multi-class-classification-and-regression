##**BITS F464 - Semester 1 - MACHINE LEARNING**
--------------------------------------------------------------------------------

**PROJECT - MACHINE LEARNING FOR SUSTAINABLE DEVELOPMENT GOALS (SDGs)**
--------------------------------------------------------------------------------
***Team number: 36***

---
Name: \\
***Rishabh Patil, Kaustubh Mahatme, Harshit Agarwal, Shivam Verma, Vaishnavi Shreshthi***

---
ID: \\
***2021A7PS0464H,2021A7PS0312H,2021A7PS0247H,2021A3PS0779H,2020AAPS2203H***

# **_1. Preprocessing of Dataset_**


---


import warnings
warnings.filterwarnings('ignore')

# Import the numpy and pandas package
import numpy as np
import pandas as pd

# Data Visualisation
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('Carbon Emissions.csv')
df_original=pd.read_csv('Carbon Emissions.csv')
df.head()
df.isnull().sum()*100/df.shape[0]
#Label Encoding
from sklearn.preprocessing import LabelEncoder
# Specify the names of the features you want to label encode
features_to_label_encode = ['Make', 'Model', 'Vehicle Class', 'Transmission', 'Fuel Type']  # Replace with the feature names

label_encoder = LabelEncoder()

for feature in features_to_label_encode:
    df[feature] = label_encoder.fit_transform(df[feature])

df
To encode the CO2 emission to binary valueswe need some reference values to compare to. Following the regulations standards followed in the EU and the Canadian regulations we find that: \\
"Emissions of 100g/km or lower means you pay no tax. 150g/km is considered low. 160 to 255g/km is considered medium. Above 255g/km is considered high."

Hence the safe limit is set as 255(g/km). So to incentivise lower emissions we encode lower emissions as 1 and higher ones as 0. (1-> passed the test for emissions).
new_column_name = 'BestModel'
new_column_data = []

for index, row in df.iterrows():
    existing_feature_value = row['CO2 Emissions(g/km)']

    # Defining condition for inserting values into the new column
    if existing_feature_value <= 255:
        new_value = 1
    else:
        new_value = 0

    new_column_data.append(new_value)

# Adding the new column to the DataFrame
df[new_column_name] = new_column_data
Here our outcome variable is `BestModel`
df.head()
df.to_csv('Carbon Emission PrePr.csv')
X = df.drop(columns=['BestModel'])
Y = df['BestModel']
print(X)
print(Y)
Normalizing our data:
columns_to_normalize = X.columns

# normalizing each column using min_max technique
for column in columns_to_normalize:
    min_value = X[column].min()
    max_value = X[column].max()
    X[column] = (X[column] - min_value) / (max_value - min_value)

print(X)
print(Y)
# Calculating the correlation matrix
correlation_matrix = df.corr()

# Seting the threshold for correlation
correlation_threshold = 0.85  # You can adjust this threshold based on your requirements

# Finding highly correlated features
highly_correlated_features = set()
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:
            colname = correlation_matrix.columns[i]
            highly_correlated_features.add(colname)

# Droping highly correlated features
df_selected_features = df.drop(columns=highly_correlated_features)

print("Selected Features:")
print(df_selected_features.columns)

From our analysis of features through correlation matrix, we found the above features to be significant to our outcome.
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5)
plt.show()
#identify the influence of different variables on the emission of CO2
From the above Correlation Matrix we infer that Co2 Emissions are heavily dependent on the following features:


*   Engine Size(L)
*   Cylinders
*   Fuel Consumptions City (L/100 km)
*   Fuel Consumptions Hwy (L/100 km)
*   Fuel Consumptions Comb (L/100 km)


df_selected_features.head()
X = df_selected_features.drop(columns=['BestModel'])
Y = df_selected_features['BestModel']
print(X)
print(Y)
columns_to_normalize = X.columns

# normalize each column using min_max technique
for column in columns_to_normalize:
    min_value = X[column].min()
    max_value = X[column].max()
    X[column] = (X[column] - min_value) / (max_value - min_value)

print(X)
print(Y)
Now we'll start training our model.\\
We need to split the data into train and test for the same.
#spliting the training data and testing data
split_ratio = 0.8
split_index = int(split_ratio * len(X))
x_train, x_test = X[:split_index], X[split_index:]
y_train, y_test = Y[:split_index], Y[split_index:]

print(x_train.shape)
print(x_test.shape)
# ***2.Perceptron***
##Theorotical explanation:
The perceptron is a simple binary classification algorithm. \\ The basic equation for the working of perceptron is as follows:
The perceptron computes the weighted sum of its input features:\\
$$
z=\sum_{i=1}^n\left(w_i \cdot x_i\right)+b
$$
The activation function
$$
f(z)=\left\{\begin{array}{lc}
1, & \text { if } z>0.5 \\
0, & \text { otherwise }
\end{array}\right.
$$
During the training phase, the perceptron is presented with input samples, and the weights are adjusted based on the error in its predictions.
The update rule for each weight is given by:  \\
$$
w_{i(new)} = w_{i(old)} + \alpha (y-\hat y )x_i
$$


$$
b_{new} = b_{old} + \alpha (y-\hat y )x_i
$$
## Code
import numpy as np
import matplotlib.pyplot as plt

class Perceptron:
    def __init__(self, learning_rate=0.01, epochs=1500):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = None
        self.bias = None
        self.losses = []                        # To store training loss at each epoch
        self.accuracies = []                    # To store training accuracy at each epoch

    def initialize_weights(self, num_features):
        self.weights = np.zeros(num_features)   # Intitialises the weight to 0
        self.bias = 0                           # Intitialises the bias to 0

    def activate(self, z):                      # Activation Function
        return 1 if z > 0.5 else 0

    def predict(self, X):                      # Caluculates prediction
        z = np.dot(X, self.weights) + self.bias
        return self.activate(z)

    def _update_weights(self, X, y):            #Update the weigths and bias
        prediction = self.predict(X)
        update = self.learning_rate * (y - prediction)
        self.weights += update * X
        self.bias += update

    def train(self, X, y):
        num_samples, num_features = X.shape
        self.initialize_weights(num_features)

        for epoch in range(self.epochs):
            for i in range(num_samples):
                self._update_weights(X[i], y[i])

            # Compute training loss and accuracy at each epoch
            predictions = [self.predict(sample) for sample in X]
            loss = np.mean(np.abs(predictions - y))
            accuracy = np.mean(predictions == y)

            # Store loss and accuracy
            self.losses.append(loss)
            self.accuracies.append(accuracy)

    def evaluate(self, X, y):
        predictions = [self.predict(sample) for sample in X]
        accuracy = np.mean(predictions == y)
        return accuracy

    def confusion_matrix(self, X, y):
        # Initialize variables to count true positives, true negatives, false positives, and false negatives
        tp = tn = fp = fn = 0

        # Iterating through the samples
        for i in range(len(X)):
            prediction = self.predict(X[i])
            actual = y[i]

            # Updating confusion matrix counts
            if actual == 1 and prediction == 1:
                tp += 1
            elif actual == 0 and prediction == 0:
                tn += 1
            elif actual == 0 and prediction == 1:
                fp += 1
            elif actual == 1 and prediction == 0:
                fn += 1

        return np.array([[tn, fp], [fn, tp]])
x_train_np = x_train.to_numpy()
y_train_np = y_train.to_numpy()

x_test_np = x_test.to_numpy()
y_test_np = y_test.to_numpy()

# Initializing and training the Perceptron model
perceptron_model = Perceptron(learning_rate=0.01, epochs=1500)
perceptron_model.train(x_train_np, y_train_np)

# Calculating accuracies on training and testing set
accuracy_train = perceptron_model.evaluate(x_train_np, y_train_np)
accuracy_test = perceptron_model.evaluate(x_test_np, y_test_np)


print(f"Model Accuracy on Train Set: {accuracy_train*100}")
print(f"Model Accuracy on Test Set: {accuracy_test*100}")
## Analysis
# Ploting the training loss curve
plt.plot(perceptron_model.losses)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss Curve')
plt.show()
# Ploting the model accuracy curve
plt.plot(perceptron_model.accuracies)
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy Curve')
plt.show()
# Confusion matrix
confusion_matrix_perceptron = perceptron_model.confusion_matrix(x_test_np, y_test_np)

# Ploting the confusion matrix
labels = ['0', '1']
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix_perceptron, annot=True, fmt='d', cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
## Predicting "best" car models
We'll select the best car models based on the number of times they pass the emission test, i.e their 1 frequency in test.
X_np = X.to_numpy()
predictions = [perceptron_model.predict(sample) for sample in X_np]

df_with_predictions = df_original.copy() #Note:the data is not normalizd here
df_with_predictions['Predictions'] = predictions

# Group by car model and count the number of predicted 1s for each model
model_counts = df_with_predictions.groupby('Model')['Predictions'].sum().reset_index()

# Sort models based on the number of predicted 1s in descending order
sorted_models = model_counts.sort_values(by='Predictions', ascending=False)

# Display the top-ranked car models
top_n_models = 10
print(f"Top {top_n_models} Car Models with Maximum Predicted 1s:")
print(sorted_models.head(top_n_models))
These are the consistently best car models
# ***3.Fischer's Linear Discrimant Analysis***


## Theoritical Explanation
Fisher's Linear Discriminant Analysis (LDA) is a dimensionality reduction technique that seeks to find the linear combination of features that best separates two or more classes.
he goal is to maximize the distance between the means of different classes while minimizing the spread (variance) within each class.
$$
\operatorname{Maximize} J(W)=\frac{\operatorname{det}\left(W^T S_B W\right)}{\operatorname{det}\left(W^T S_W W\right)}
$$
The solution is obtained by solving the generalized eigenvalue problem:
$$
S_B v=\lambda S_W v
$$
Within Class Matrix
$$
S_W=\sum_{i=1}^C \sum_{j=1}^{n_i}\left(x_{i j}-m_i\right) \cdot\left(x_{i j}-m_i\right)^T
$$

Between-class Scatter Matrix:
$$
S_B=\sum_{i=1}^C N_i \cdot\left(m_i-m\right)^T \cdot\left(m_i-m\right)
$$

## Code
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sympy as sp

class FisherLinearDiscriminant:
    def __init__(self):
        self.w = None  # Weight vector
        self.b = None  # Bias term

    def calc_threshold(self, m0, m1, pi1, pi2, sw_0, sw_1):
        # Function to calculate the threshold for Fisher's Linear Discriminant
        x = sp.Symbol('x')

        # Calculate the inverse of the sum of within-class scatter matrices
        sw_inv = np.linalg.inv(sw_0 + sw_1)

        # Calculate the difference in means between the two classes
        diff_means = m1 - m0

        # Calculate the optimal weight vector
        w = np.dot(sw_inv, diff_means)

        # Calculate the threshold based on the means and weight vector
        thr = 0.5 * np.dot((m0 + m1), w)
        return thr

    def fishers(self, X, Y, x_train, y_train, x_test, y_test):
        # Function to perform Fisher's Linear Discriminant
        self.unique_classes = np.unique(y_train)

        # Separate data into two classes
        X = X.values
        X_0 = x_train[y_train == 0]
        X_1 = x_train[y_train == 1]

        # Calculate class means
        mean_0 = np.mean(x_train[y_train == 0], axis=0)
        mean_1 = np.mean(x_train[y_train == 1], axis=0)

        # Calculate class scatter matrices
        Sw_0 = np.dot((x_train[y_train == 0] - mean_0).T, x_train[y_train == 0] - mean_0)
        Sw_1 = np.dot((x_train[y_train == 1] - mean_1).T, x_train[y_train == 1] - mean_1)

        # Calculate within-class scatter matrix
        Sw = Sw_0 + Sw_1

        # Calculate optimal weight vector
        self.w = np.dot(np.linalg.inv(Sw), (mean_1 - mean_0))
        pi0 = X_0.shape[0] / (X_0.shape[0] + X_1.shape[0])
        pi1 = 1 - pi0

        # Calculate bias term
        self.b = self.calc_threshold(mean_0, mean_1, pi1, pi0, Sw_0, Sw_1)

        # Make predictions on the test set
        result = self.predict(self.w, x_test, self.b)

        # Calculate accuracy
        accuracy = self.check_accuracy(result, y_test)
        return accuracy

    def predict(self, w, x_test, thr):
        # Function to make predictions using the Fisher's Linear Discriminant model
        y = np.dot(x_test, w)
        result = (y >= thr).astype(int)
        return result

    def check_accuracy(self, result, y_test):
        # Function to calculate accuracy
        accuracy = np.mean(result == y_test) * 100
        return accuracy

    def confusionMatrix(self, X_test, y_test):
        # Get predictions using the predict method
        y_pred = self.predict(self.w, x_test, self.b)

        # Create a confusion matrix
        num_classes = len(self.unique_classes)
        confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)

        # Populate the confusion matrix
        for actual, predicted in zip(y_test, y_pred):
            actual_idx = np.where(self.unique_classes == actual)[0][0]
            predicted_idx = np.where(self.unique_classes == predicted)[0][0]
            confusion_matrix[actual_idx, predicted_idx] += 1

        plt.figure(figsize=(8, 6))
        sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=self.unique_classes, yticklabels=self.unique_classes)
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title('Confusion Matrix')
        plt.show()

    def plot_predicted_vs_input(self, x_test, x_dimension_to_plot):
        # Make predictions
        y = self.predict(self.w, np.array(x_test), self.b)  # Convert x_test to a NumPy array

        # Map numerical labels to "Safe" and "Unsafe"
        labels = {0: "Unsafe", 1: "Safe"}
        y_labels = np.vectorize(labels.get)(y)

        # Define colors for "Safe" and "Unsafe"
        colors = {0: "red", 1: "blue"}
        y_colors = np.vectorize(colors.get)(y)

        # Plot the data with labels and specified colors
        n = x_dimension_to_plot
        column_heading = x_test.columns[n]

        plt.scatter(np.array(x_test)[:, x_dimension_to_plot], y, c=y_colors, cmap='viridis')  # Convert x_test to a NumPy array
        plt.xlabel(column_heading)
        plt.ylabel('Prediction')
        plt.title('Vehicle Safety Prediction')
        plt.yticks([0, 1], labels.values())  # Set y-axis ticks to "Unsafe" and "Safe"
        plt.show()




## Analysis
fischer_model = FisherLinearDiscriminant()
accuracy = fischer_model.fishers(X,Y,x_train, y_train, x_test, y_test)
print(f"Accuracy: {accuracy}%")
confusion_matrix_fischer = fischer_model.confusionMatrix(x_test, y_test)
for n in range(x_test.shape[1]):
    print(f"Scatter matrix of Y vs attribute number {n+1}")
    fischer_model.plot_predicted_vs_input(x_test, x_dimension_to_plot=n)
    print()  # Add an empty line after each iteration

## Predicting "best" car models
We merge the predicted outcome column to our dataset.
# Predictions on the full dataset
predictions = fischer_model.predict(fischer_model.w, X, fischer_model.b)

# Combine predictions with the dataset
df_with_predictions = df_original.copy()
df_with_predictions['Predictions'] = predictions

# Group by car model and count the number of predicted 1s for each model
model_counts = df_with_predictions.groupby('Model')['Predictions'].sum().reset_index()

# Sort models based on the number of predicted 1s in descending order
sorted_models = model_counts.sort_values(by='Predictions', ascending=False)

# Display the top-ranked car models
top_n_models = 10
print(f"Top {top_n_models} Car Models with Maximum Predicted 1s:")
print(sorted_models.head(top_n_models))
# ***4. Naive Bayes***
## Theoritical Explanation
Naive Bayes assumes that the features are conditionally independent given the class label. This assumption simplifies the likelihood term:
$$
\hat y = \text{arg max}_y P(y|X)
$$
$$
P\left(x_i \mid y\right)=\frac{1}{\sqrt{2 \pi \sigma_y^2}} \exp \left(-\frac{\left(x_i-\mu_y\right)^2}{2 \sigma_y^2}\right)
$$
$$
P(y) = \frac{\text{No. of instances of class y}} {\text{Total No. of instances}}
$$
## Code
# Visualize the class probability distribution for each class. This can be done by plotting the estimated probability densities for each class.
def class_probability_distribution(X, y, num_feature):
    unique_classes = np.unique(y)

    plt.figure(figsize=(24,20))

    for class_label in unique_classes:
        class_features = X[y == class_label]

        for feature_idx in range(num_feature):
            plt.subplot(num_feature, len(unique_classes), class_label * num_feature + feature_idx + 1)
            sns.histplot(class_features[:, feature_idx], kde=True, label=f'Class {class_label}', color='skyblue', stat='density')

            plt.title(f'Class {class_label} - Feature {feature_idx}')
            plt.xlabel('Feature Value')
            plt.ylabel('Density')
            plt.legend()

    plt.tight_layout()
    plt.show()

class_probability_distribution(x_train.to_numpy(), y_train, num_feature=x_train.shape[1])

class GNaiveBayesClassifier:

    def priorProb(self, X, y):
      # self is a reference to the instance of a class. It is used to access the attributes and methods of the class.

        grouped_data = X.groupby(y).apply(lambda x: len(x))
        # Calculate the prior probability by dividing the length of each group by the total number of rows
        prior_prob = grouped_data / self.num_rows
        # Convert the prior probability to a numpy array
        self.prior = prior_prob.to_numpy()
        return self.prior

    def stat_Paramteres(self, X, y):

        #  Group the data by y and calculate the mean of each group
        grouped_mean = X.groupby(y).apply(np.mean)
        #  Convert the grouped mean to a numpy array
        self.mean = grouped_mean.to_numpy()
        #  Group the data by y and calculate the variance of each group
        grouped_var = X.groupby(y).apply(np.var)
        #  Convert the grouped variance to a numpy array
        self.var = grouped_var.to_numpy()
        #  Return the mean and variance
        return self.mean, self.var

    def dens_Gauss(self, class_i, x):

        #  Retrieve the mean and variance of the specified class
        mean = self.mean[class_i]
        var = self.var[class_i]
        #  Calculate the probability mass function (PMF) using the mean, variance, and input value x
        pmf = np.exp((-1/2)*((x-mean)**2) / (2 * var)) / np.sqrt(2 * np.pi * var)

        return pmf

    def post_Prob(self, x):

        posteriors = []

        for i in range(self.count_unique_classes):

            prior = np.log(self.prior[i])
            conditional = np.sum(np.log(self.dens_Gauss(i, x)))
            posterior = prior + conditional
            posteriors.append(posterior)

        return self.unique_classes[np.argmax(posteriors)]

    def train(self, X, y):

        # Get unique classes and their count
        self.unique_classes = np.unique(y)
        self.count_unique_classes = len(self.unique_classes)
        # Get number of features and rows
        self.num_feature = X.shape[1]
        self.num_rows = X.shape[0]
        # Calculate statistical parameters
        self.stat_Paramteres(X, y)
        # Calculate prior probabilities
        self.priorProb(X, y)

    def predict(self, X):
        # preds = [self.post_Prob(f) for f in X.to_numpy()]
        preds = []
        for f in X.to_numpy():
          # For each row, call the post_Prob function with the current row as an argument.
          # Append the result of post_Prob to the preds list.
          preds.append(self.post_Prob(f))
        return preds

    def accuracy(self, y_test, y_pred):
        # accuracy = np.sum(y_test == y_pred) / len(y_test)

        # Calculate the number of correct predictions by comparing y_test and y_pred using the == operator.
        correct_predictions = np.sum(y_test == y_pred)
        total_predictions = len(y_test)
        accuracy = correct_predictions / total_predictions
        return accuracy

    def confusionMatrix(self, X_test, y_test):
        # Get predictions using the predict method
        y_pred = self.predict(X_test)

        # Create a confusion matrix
        num_classes = len(self.unique_classes)
        confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)

        # Populate the confusion matrix
        for actual, predicted in zip(y_test, y_pred):
            actual_idx = np.where(self.unique_classes == actual)[0][0]
            predicted_idx = np.where(self.unique_classes == predicted)[0][0]
            confusion_matrix[actual_idx, predicted_idx] += 1

        plt.figure(figsize=(8, 6))
        sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=self.unique_classes, yticklabels=self.unique_classes)
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title('Confusion Matrix')
        plt.show()


        return confusion_matrix
## Analysis
naive_bayes_model = GNaiveBayesClassifier()
naive_bayes_model.train(x_train, y_train)

predicted = naive_bayes_model.predict(x_test)
accuracy = naive_bayes_model.accuracy(y_test, predicted)*100
print(f"Accuracy: {accuracy}%")
confusion_matrix_naive_bayes = naive_bayes_model.confusionMatrix(x_test, y_test)
## Predicting "best" car models
X_np = pd.DataFrame(X)

# Make predictions using the trained perceptron model
predictions = naive_bayes_model.predict(X_np)

# Combine predictions with the dataset
df_with_predictions = df_original.copy() #Note:the data is not normalizd here
df_with_predictions['Predictions'] = predictions

# Group by car model and count the number of predicted 1s for each model
model_counts = df_with_predictions.groupby('Model')['Predictions'].sum().reset_index()

# Sort models based on the number of predicted 1s in descending order
sorted_models = model_counts.sort_values(by='Predictions', ascending=False)

# Display the top-ranked car models
top_n_models = 10
print(f"Top {top_n_models} Car Models with Maximum Predicted 1s:")
print(sorted_models.head(top_n_models))

# ***5. Selecting a researched model for our dataset***
## Testing various models to choose best amongst them
Firstly analysing the sutable model to code for later.

#note that we used the librarires only for prelimnary analysis, the code for our selected model is presented below later.
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load your dataset (replace 'your_dataset.csv' with your actual file path)
df = pd.read_csv('Carbon Emissions.csv')

df
X = df.drop('CO2 Emissions(g/km)', axis=1)
y = df['CO2 Emissions(g/km)']
Testing various variables for selecting the best amongst them.
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define categorical and numerical features
categorical_features = ['Make', 'Model', 'Vehicle Class', 'Transmission', 'Fuel Type']
numerical_features = ['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', 'Fuel Consumption Hwy (L/100 km)',
                      'Fuel Consumption Comb (L/100 km)', 'Fuel Consumption Comb (mpg)']

numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])


# Define models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(random_state=42),
    'k-Nearest Neighbors': KNeighborsRegressor(),
    'Support Vector Machine': SVR(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'XGBoost': XGBRegressor(objective ='reg:squarederror')
}

# Iterate over models and evaluate
for model_name, model in models.items():
    # Create a pipeline with the preprocessor and the model
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('model', model)])

    # Train the model
    pipeline.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = pipeline.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f'{model_name} Metrics:')
    print(f'Mean Squared Error: {mse}')
    print(f'Mean Absolute Error: {mae}')
    print(f'R-squared: {r2}')
    print('--------------------------')

From our prelimnary analysis we can see that Random Forest has the best metrics among the 6. But since we cannot choose Random Forest, we'll see others' metrics. \\
XGBoost, Gradient Boosting, have very comparable stats, and we can choose XGBoost as it has a slightly lower Mean Absolute Error and slightly higher R^2 value.

## XGBoost

XGBoost objective is a function of functions. \\
The objective function (loss function and regularization) at iteration $t$ that we need to minimize is the following:
$$
\mathcal{L}^{(t)}=\sum_{i=1}^n l\left(y_i, \hat{y}_i^{(t-1)}+f_t\left(\mathbf{x}_i\right)\right)+\Omega\left(f_t\right)
$$
here $y_i$ is the real value(label) known from the training data-set.
extArr = []
### Code
from collections import defaultdict
import math
class XGBoostModel():
    gainArrm=[]
    def __init__(self, params, random_seed=None):
        self.params = defaultdict(lambda: None, params)
        self.subsample = self.params['subsample'] \
            if self.params['subsample'] else 1.0
        self.learning_rate = self.params['learning_rate'] \
            if self.params['learning_rate'] else 0.3
        self.base_prediction = self.params['base_score'] \
            if self.params['base_score'] else 0.5
        self.max_depth = self.params['max_depth'] \
            if self.params['max_depth'] else 5
        self.rng = np.random.default_rng(seed=random_seed)

    def fit(self, X, y, objective, num_boost_round, verbose=False):
        current_predictions = self.base_prediction * np.ones(shape=y.shape)
        self.boosters = []
        for i in range(num_boost_round):
            gradients = objective.gradient(y, current_predictions)
            hessians = objective.hessian(y, current_predictions)
            sample_idxs = None if self.subsample == 1.0 \
                else self.rng.choice(len(y),
                                     size=math.floor(self.subsample*len(y)),
                                     replace=False)
            booster = TreeBooster(X, gradients, hessians,
                                  self.params, self.max_depth, sample_idxs)
            current_predictions += self.learning_rate * booster.predict(X)
            self.boosters.append(booster)
            gainArrm=booster.gainRet()
            if verbose:
                print(f'[{i}] train loss = {objective.loss(y, current_predictions)}')

    def predict(self, X):
        return (self.base_prediction + self.learning_rate
                * np.sum([booster.predict(X) for booster in self.boosters], axis=0))

    def gainRetrieval(self):
        return self.gainArrm

class TreeBooster():
    gainArr=[]

    def __init__(self, X, g, h, params, max_depth, idxs=None):

        self.params = params
        self.max_depth = max_depth
        assert self.max_depth >= 0, 'max_depth must be nonnegative'
        self.min_child_weight = params['min_child_weight'] \
            if params['min_child_weight'] else 1.0
        self.reg_lambda = params['reg_lambda'] if params['reg_lambda'] else 1.0
        self.gamma = params['gamma'] if params['gamma'] else 0.0
        self.colsample_bynode = params['colsample_bynode'] \
            if params['colsample_bynode'] else 1.0
        if isinstance(g, pd.Series): g = g.values
        if isinstance(h, pd.Series): h = h.values
        if idxs is None: idxs = np.arange(len(g))
        self.X, self.g, self.h, self.idxs = X, g, h, idxs
        self.n, self.c = len(idxs), X.shape[1]
        self.value = -g[idxs].sum() / (h[idxs].sum() + self.reg_lambda) # Eq (5)
        self.best_score_so_far = 0.
        if self.max_depth > 0:
            self._maybe_insert_child_nodes()


    def _maybe_insert_child_nodes(self):
        for i in range(self.c): self._find_better_split(i)
        if self.is_leaf: return
        x = self.X.values[self.idxs,self.split_feature_idx]
        left_idx = np.nonzero(x <= self.threshold)[0]
        right_idx = np.nonzero(x > self.threshold)[0]
        self.left = TreeBooster(self.X, self.g, self.h, self.params,
                                self.max_depth - 1, self.idxs[left_idx])
        self.right = TreeBooster(self.X, self.g, self.h, self.params,
                                 self.max_depth - 1, self.idxs[right_idx])

    @property
    def is_leaf(self): return self.best_score_so_far == 0.

    def _find_better_split(self, feature_idx):
        x = self.X.values[self.idxs, feature_idx]
        g, h = self.g[self.idxs], self.h[self.idxs]
        sort_idx = np.argsort(x)
        sort_g, sort_h, sort_x = g[sort_idx], h[sort_idx], x[sort_idx]
        sum_g, sum_h = g.sum(), h.sum()
        sum_g_right, sum_h_right = sum_g, sum_h
        sum_g_left, sum_h_left = 0., 0.

        for i in range(0, self.n - 1):
            g_i, h_i, x_i, x_i_next = sort_g[i], sort_h[i], sort_x[i], sort_x[i + 1]
            sum_g_left += g_i; sum_g_right -= g_i
            sum_h_left += h_i; sum_h_right -= h_i
            if sum_h_left < self.min_child_weight or x_i == x_i_next:continue
            if sum_h_right < self.min_child_weight: break

            gain = 0.5 * ((sum_g_left**2 / (sum_h_left + self.reg_lambda))
                            + (sum_g_right**2 / (sum_h_right + self.reg_lambda))
                            - (sum_g**2 / (sum_h + self.reg_lambda))
                            ) - self.gamma/2 # Eq(7) in the xgboost paper
            self.gainArr.append(gain)
            extArr.append(gain)
            if gain > self.best_score_so_far:
                self.split_feature_idx = feature_idx
                self.best_score_so_far = gain
                self.threshold = (x_i + x_i_next) / 2

    def predict(self, X):
        return np.array([self._predict_row(row) for i, row in X.iterrows()])

    def gainRet(self):
      return self.gainArr

    def _predict_row(self, row):
        if self.is_leaf:
            return self.value
        child = self.left if row[self.split_feature_idx] <= self.threshold \
            else self.right
        return child._predict_row(row)

class SquaredErrorObjective():
    def loss(self, y, pred): return np.mean((y - pred)**2)
    def gradient(self, y, pred): return pred - y
    def hessian(self, y, pred): return np.ones(len(y))

params = {
    'learning_rate': 0.1,
    'max_depth': 5,
    'subsample': 0.8,
    'reg_lambda': 1.5,
    'gamma': 0.0,
    'min_child_weight': 25,
    'base_score': 0.0,
    'tree_method': 'exact',
}
num_boost_round = 50
X_train = X_train[numerical_features]
X_train
X_test = X_test[numerical_features]
model_scratch = XGBoostModel(params, random_seed=42)
model_scratch.fit(X_train, y_train, SquaredErrorObjective(), num_boost_round)
pred_scratch = model_scratch.predict(X_test)
print(f'scratch score: {SquaredErrorObjective().loss(y_test, pred_scratch)}')
import matplotlib.pyplot as plt
plt.plot(extArr)
**Using preprocessed csv file to incorporate categorical variables:**

dfpp = pd.read_csv('Carbon Emission PrePr.csv')
Xpp = dfpp.drop(['CO2 Emissions(g/km)','Unnamed: 0','BestModel'], axis=1)
ypp = dfpp['BestModel']
Xpp_train, Xpp_test, ypp_train, ypp_test = train_test_split(Xpp, ypp, test_size=0.2, random_state=42)
Xpp_train
Xpp = dfpp.drop(['BestModel','Unnamed: 0','BestModel'], axis=1)
ypp = dfpp['CO2 Emissions(g/km)']
Xpp_train, Xpp_test, ypp_train, ypp_test = train_test_split(Xpp, ypp, test_size=0.2, random_state=42)
Xpp_train
np.size(Xpp['Make'].unique())
ypp
model_scratch = XGBoostModel(params, random_seed=42)
model_scratch.fit(Xpp_train, ypp_train, SquaredErrorObjective(), num_boost_round)
pred_scratch = model_scratch.predict(Xpp_test)
print(f'scratch score: {SquaredErrorObjective().loss(ypp_test, pred_scratch)}')
### Analysis
Plotting the gain value through each iteration:
plt.plot(extArr)
plt.plot(extArr[1:450000])
extArr[-1]
**Accuracy Metrics for regressive model**:
mae = np.mean(np.abs(ypp_test - pred_scratch))
mse = np.mean((ypp_test - pred_scratch)**2)
rmse = np.sqrt(mse)
mean_y = np.mean(ypp_test)
ss_total = np.sum((ypp_test - mean_y)**2)
ss_residual = np.sum((ypp_test - pred_scratch)**2)
r2 = 1 - (ss_residual / ss_total)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')
print(f'R-squared: {r2}')
The R-square value is significanty high.
Comparing with sklearn xgboost:

print(f'scratch score: {SquaredErrorObjective().loss(y_test, pred_scratch)}')
import xgboost as xgb
dtrain = xgb.DMatrix(Xpp_train, label=ypp_train)
dtest = xgb.DMatrix(Xpp_test, label=ypp_test)
model_xgb = xgb.train(params, dtrain, num_boost_round)
pred_xgb = model_xgb.predict(dtest)
print(f'xgboost score: {SquaredErrorObjective().loss(ypp_test, pred_xgb)}')
The squared Error score is very similar with no significant difference, hence the custom code is aptly accurate
### Predicting "best" car models
Checking the least polluting car models
pred_full= model_scratch.predict(Xpp)
pred_full.max()
# Combine predictions with the dataset
df_with_predictions = df_original=df.copy() #Note:the data is not normalizd here
df_with_predictions['Predictions'] = pred_full
df_with_predictions
# Group by car model and count the number of predicted 1s for each model
model_counts = df_with_predictions.groupby('Model')['Predictions'].mean().reset_index()

# Sort models based on the number of predicted 1s in descending order
sorted_models = model_counts.sort_values(by='Predictions')

# Display the top-ranked car models
top_n_models = 10
print(f"Top {top_n_models} Car Models with Maximum Predicted 1s:")
print(sorted_models.head(top_n_models))
From the above analysis from the deployed model of XGBoost shows that the follwing car models have the least emission. This is a considerebaly good estimate as the predicted low emission cars are hybrid vehicles, which in general sense should be the desired outcome.
# ***6. Comparison of insights drawn from the models***
Top 10 Car Models with Maximum Predicted From Perceptron:




---

FOCUS FFV  
  SONIC 5  
    SONIC  
    JETTA  
  COMPASS  
  COROLLA  
  FORTE 5  
      ATS  
  PATRIOT  
   ACCORD  
Top 10 Car Models with Maximum Predicted From Fischer :
            
FOCUS FFV  
  SONIC 5  
    SONIC  
      ATS  
   ACCORD  
  COMPASS  
    JETTA  
  FORTE 5  
  COROLLA  
  PATRIOT  
Top 10 Car Models with Maximum Predicted From Naive-Bayes:


---


      
FOCUS FFV  
    SONIC  
  SONIC 5  
      ATS  
    JETTA  
  COMPASS  
   ACCORD  
  COROLLA  
  FORTE 5  
  PATRIOT          
All the three models are consistent in predicting the best car models, hence the consistency is aply significant.
Top 10 Car Models with Maximum Predicted From XGBoost:

---
IONIQ BLUE   
IONIQ Blue  
     IONIQ  
   NIRO FE  
   Niro FE  
 Corolla Hybrid  
      Prius AWD  
          Prius  
          PRIUS  
Camry Hybrid LE  
The accuracy metrics for each model is:
1. Perceptron: 89.91198375084631%
2. Fischer LDA: 90.18280297901151%
3. Naive Bayes: 85.17264725795532%
4. XGBoost: R^2 metric: 0.996
# **_7. References_**




## **1. Naive Bayes Classifier**
Class Notes

[Towards Data Science](https://towardsdatascience.com/the-naive-bayes-classifier-how-it-works-e229e7970b84#:~:text=The%20Naive%20Bayes%20algorithm%20is%20explained%20through%20simple%20examples.&text=Introduction%3A,%2Fmedium%2Fhigh%2C%20etc.)

[Andrew NG Playlist](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)

---
## **2. Perceptron**
Class Notes

Andrew NG Notes

[Analytics-Vidhya](https://medium.com/analytics-vidhya/the-perceptron-algorithm-for-binary-classification-ab65aaf237d7)

[AssemblyAI](https://www.youtube.com/watch?v=aOEoxyA4uXU)

---
## **3. Fischer Discriminant Analysis**



Class Notes

[Andrew NG Playlist](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)

[Stack Exchange](https://stats.stackexchange.com/questions/4942/threshold-for-fisher-linear-classifier
)

[PlainEnglish](https://ai.plainenglish.io/fischers-linear-discriminant-analysis-in-python-from-scratch-bbe480497504
)

---
## **4. XGBoost**

Class Notes

[Andrew NG Playlist](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)

[Original Reference Paper](https://arxiv.org/abs/1603.02754)

[kdnuggets](https://www.kdnuggets.com/2018/08/unveiling-mathematics-behind-xgboost.html
)

---
